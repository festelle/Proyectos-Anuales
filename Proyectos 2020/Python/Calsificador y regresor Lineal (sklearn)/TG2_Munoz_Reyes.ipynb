{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TG2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0S7JT_kn8DY",
        "colab_type": "text"
      },
      "source": [
        "Clasificador y regresor lineal mediante SKlearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgSmeVE-2WgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import altair as alt\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "#Se prepara todo lo necesario para hacer el clasificador\n",
        "from sklearn import neighbors\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBIt8sIEjMFp",
        "colab_type": "text"
      },
      "source": [
        "1- Análisis de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT7Bcob24Xrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "juegos=pd.read_csv(\"games.csv\",encoding='ISO-8859-1')\n",
        "print(juegos.columns)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptWMFDwT6peS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Solo importa editora del juego y ventas en el resto del mundo, por lo que se eliminan los datos que no estén relacionados.\n",
        "del juegos['Id']\n",
        "del juegos['Name']\n",
        "del juegos['Rank']\n",
        "del juegos['Difficulty']\n",
        "del juegos['Year']\n",
        "juegos.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2XBwIZu8et_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Primero se verá en qué columnas hay valores nulos\n",
        "\n",
        "print(juegos['Publisher'].isnull().values.sum())\n",
        "print(juegos['Platform'].isnull().values.sum())\n",
        "print(juegos['Genre'].isnull().values.sum())\n",
        "print(juegos['Ign_score'].isnull().values.sum())\n",
        "print(juegos['NA_Sales'].isnull().values.sum())\n",
        "print(juegos['EU_Sales'].isnull().values.sum())\n",
        "print(juegos['JP_Sales'].isnull().values.sum())\n",
        "print(juegos['Other_Sales'].isnull().values.sum())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf-2TGBq6dN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Se van a eliminar todas las filas que no contengan ningún dato sobre ventas, al ser consideradas no útiles.\n",
        "#Además, se elimina la columna de Ign_score porque más del 60% de datos son nulos.\n",
        "del juegos['Ign_score']\n",
        "juegos.dropna(subset=['NA_Sales','EU_Sales','JP_Sales','Other_Sales'],how='all',inplace=True)\n",
        "#Se revisa ahora qué datos nulos quedan\n",
        "print('Nulos en Platform:',juegos['Platform'].isnull().values.sum())\n",
        "print('Nulos en Publisher:',juegos['Publisher'].isnull().values.sum())\n",
        "print('Nulos en NA_Sales:',juegos['NA_Sales'].isnull().values.sum())\n",
        "print('Nulos en EU_Sales',juegos['EU_Sales'].isnull().values.sum())\n",
        "print('Nulos en JP_Sales:',juegos['JP_Sales'].isnull().values.sum())\n",
        "print('Nulos en Other_Sales',juegos['Other_Sales'].isnull().values.sum())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9wAlOjYmq8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Se eliminan las filas que sean nulos en Publisher, porque este es uno de los datos principales para poder entrenar al programa después, además de ser\n",
        "#menos del 0.1%\n",
        "juegos.dropna(subset=['Publisher'],inplace=True)\n",
        "print(juegos['Publisher'].isnull().values.sum())\n",
        "\n",
        "juegos.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D20PAf6jnfZ",
        "colab_type": "text"
      },
      "source": [
        "1.1 - Codificar datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JUx4Rn_jmJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(juegos['Platform'].unique())\n",
        "print(len(juegos['Publisher'].unique()))\n",
        "juegos.groupby([\"Publisher\"]).count().median()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk1w3NiaXkWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# se crea un vector de Other_Sales y de Publisher, junto a la matriz asociada\n",
        "vectorventas = juegos['Other_Sales']\n",
        "vectorpublisher = juegos['Publisher']\n",
        "\n",
        "matriz = juegos.copy()\n",
        "\n",
        "del matriz['Publisher']\n",
        "del matriz['Other_Sales']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiqjWsKRDGm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "#Con onehotencoder se pasa la columna de platform y genre a vectores \n",
        "one = pd.get_dummies(matriz['Platform'], prefix='Platform')\n",
        "matriz = pd.concat([matriz, one], axis=1, sort=False)\n",
        "del matriz['Platform']\n",
        "\n",
        "one = pd.get_dummies(matriz['Genre'], prefix='Genre')\n",
        "matriz = pd.concat([matriz, one], axis=1, sort=False)\n",
        "del matriz['Genre']\n",
        "\n",
        "\n",
        "\n",
        "#Se normalizan las columnas\n",
        "#Este código se sacó de https://stackoverflow.com/questions/26414913/normalize-columns-of-pandas-data-frame\n",
        "from sklearn import preprocessing\n",
        "\n",
        "x = matriz.values #returns a numpy array\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit_transform(x)\n",
        "matriz = pd.DataFrame(x_scaled)\n",
        "\n",
        "#Con esto ya se tiene la matriz normalizada junto a los vectores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHpGcoPWzy2m",
        "colab_type": "text"
      },
      "source": [
        "2.1 Clasificador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnd1kRKI5f06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Se eliminó la columna Rank y mejoró el acurracy\n",
        "\n",
        "#Se va a cambiar la matriz de prueba, agregando las columna de Other_Sales para tener más información y esperar que \n",
        "#el programa mejore su precisión, resultado: Se mantuvo en 17%, se vuelve a eliminar.\n",
        "\n",
        "#Se agrega la columna de genre, además de cambiar k=50 y x_test=0.2. Resultado: Aumenta a un 20% \n",
        "#Se agrega la columna year. resultado: Disminuye a un 18%, por lo que se vuelve a eliminar.\n",
        "\n",
        "#Con la nueva matriz se prueba\n",
        "#Se cambia knn weights a distance. Empeora a un 18% de acurracy.\n",
        "#algorithm='ball_tree' resultado: mantiene 20% , algorithm='kd_tree' mantiene 20%\n",
        "#distance + brute = 18% acurracy\n",
        "# Probando con diversos valores de n, se llega a que n=50 mejora los resultados\n",
        "\n",
        "\n",
        "#Se define el clasificador\n",
        "knn = neighbors.KNeighborsClassifier(n_neighbors=50, weights='uniform', algorithm='auto')\n",
        " #Se separa la matriz y el vectorpublisher en set de entrenamiento y set de testeo\n",
        "X_train, X_test, y_train, y_test = train_test_split(matriz, vectorpublisher, test_size=0.2, random_state=0)\n",
        "#Se entrena el clasificador\n",
        "knn.fit(X_train, y_train)\n",
        "#Se revisa su comportamiento\n",
        "predicted = knn.predict(X_test)\n",
        "print( metrics.classification_report(y_test, predicted))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y57q2Ms4z5gj",
        "colab_type": "text"
      },
      "source": [
        "2.2 Regresor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgwiWpd0dESX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#La base de este código fue sacado del ejemplo de la clase 13 hecha por el profesor Denis Parra.\n",
        "\n",
        "#fit intercept= False. Resultado MSE se mantiene\n",
        "#normalize= True. Resultado MSE se mantiene (como era de esperarse)\n",
        "#n_jobes se prueba con muchos números. Resultado MSE se mantiene\n",
        "\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "#Para entrenar el regresor, se separa la matriz y la columna Other_Sales en set de entrenamiento y de testeo\n",
        "X_train, X_test, y_train, y_test = train_test_split(matriz, vectorventas, test_size=0.3, random_state=0)\n",
        "\n",
        "# Se crea el regresor\n",
        "regr = linear_model.LinearRegression( fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)\n",
        "\n",
        "# Se entrena  el regresor con los set de entrenamiento\n",
        "regr.fit(X_train, y_train)\n",
        "\n",
        "# Se hace la predicción del set de testeo\n",
        "y_pred = regr.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUXmOr-N0cSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Estas son las predicciones hechas.\n",
        "y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJQ95Z-q0oFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Se revisa el MSE, es muy bajo, por lo que la predicción es buena.\n",
        "print(mean_squared_error(y_test, y_pred))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_PohwTZ0dn4",
        "colab_type": "text"
      },
      "source": [
        "3.1 Redución de dimensionalidad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcKaw9nj0kDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Se usa tsne para reducir la dimensión\n",
        "from sklearn.manifold import TSNE\n",
        "tsne = TSNE(perplexity=50, n_iter=1000, verbose=1)\n",
        "intermediario = tsne.fit_transform(matriz)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSPKMnkkAjlS",
        "colab_type": "text"
      },
      "source": [
        "3.2 Visualizar datos reducidos "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM_ga9_G4nlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Se crea un dataframe de pandas para utilizarlo en el grafico\n",
        "tsneDataframe = pd.DataFrame(intermediario,columns=[\"tsne-1\",\"tsne-2\"])\n",
        "#Se desactiva el limite de datos de altair\n",
        "alt.data_transformers.disable_max_rows()\n",
        "#Se agrega la columna de Publisher para identificarla con colores en el grafico\n",
        "tsneDataframeAlt = tsneDataframe\n",
        "tsneDataframeAlt[\"Publisher\"] = vectorpublisher\n",
        "alt.Chart(tsneDataframeAlt).mark_circle(size=60).encode(\n",
        "    x=\"tsne-1\",\n",
        "    y=\"tsne-2\",\n",
        "    color=\"Publisher\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW1usoOJArwV",
        "colab_type": "text"
      },
      "source": [
        "3.3 Predecir las clases usando clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr9BERqmAxKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Se usa kmeans para clustering con centroides\n",
        "from sklearn.cluster import  KMeans\n",
        "print(vectorpublisher.nunique())\n",
        "#Como hay 578 Publishers se usan 578 centros\n",
        "kmeans = KMeans(n_clusters=578, verbose=0, random_state=0).fit(intermediario)\n",
        "kmeans.labels_\n",
        "print(kmeans.cluster_centers_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqio5tc4I9ia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Se usa dbscan para clustering por densidad\n",
        "from sklearn.cluster import DBSCAN\n",
        "#Se usa eps=0.5 por dar un resultado cercano al nuemero real de Publisher, lo mismo para min_saples=5, aunque este afecta menos el resultado\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5, n_jobs=4).fit(intermediario)\n",
        "dbscan.labels_\n",
        "len(set(dbscan.labels_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK_8X63sKwXC",
        "colab_type": "text"
      },
      "source": [
        "3.4 Visualizar los cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNperze2K3XM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Se agrega el label creado por kmeans como columna Publisher\n",
        "tsneDataframeAlt = tsneDataframe\n",
        "tsneDataframeAlt[\"Publisher\"] = kmeans.labels_\n",
        "alt.Chart(tsneDataframeAlt).mark_circle(size=60).encode(\n",
        "    x=\"tsne-1\",\n",
        "    y=\"tsne-2\",\n",
        "    color=\"Publisher:N\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YchGNS9NLaih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Se agrega el label creado por dbscan como columna Publisher\n",
        "tsneDataframeAlt = tsneDataframe\n",
        "tsneDataframeAlt[\"Publisher\"] = dbscan.labels_\n",
        "alt.Chart(tsneDataframeAlt).mark_circle(size=60).encode(\n",
        "    x=\"tsne-1\",\n",
        "    y=\"tsne-2\",\n",
        "    color=\"Publisher:N\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mHp6Kzpah5-",
        "colab_type": "text"
      },
      "source": [
        "4- Preguntas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flDxU7eFapyn",
        "colab_type": "text"
      },
      "source": [
        "1- Eliminaste columnas en el dataset que no aportaban información? De ser así, ¿cuáles fueron y por qué? ¿Cómo resolviste el tema de los valores nulos?\n",
        "\n",
        " Eliminamos las columnas de Id y Name, porque en este caso no nos iteresaba poder identificar cada juego individualmente. También eliminamos las columnas Year, Difficulty y Rank después de probar cada una de estas columnas en el clasificador y ver que el acurracy disminuía si se incluían. Por último, se eliminó la columna Ign_score porque más del 60% de sus datos eran nulos, por lo que estaba demasiado incompleta como para poder usarla. Para los valores nulos, se eliminaron todas las filas que no tenían ningún tipo de información sobre ventas y al final se eliminaron las filas que no tenían Publisher (siendo estas menos del 0.1%)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2- ¿Qué normalizaste, filas o columnas? ¿Por qué? ¿Para qué sirve normalizar los datos?\n",
        "\n",
        "  Se normalizaron las columnas, porque los datos se tienen que normalizar con respecto a los de su misma clase y se normalizó porque así todos los datos podían tener la misma importancia. Normalizar los datos sirve para que no haya un tipo de dato que tenga mayor peso que otro. Por ejemplo, si en la regresión la matriz no se hubiera normalizado, las columnas con información de ventas (que tienen valores desde 0 hasta 41.49) tendrían mucho más peso que las columnas de Genre y Platform, al ser estas últimas con valores 0 y 1. Para que todas las categorías tengan la misma importancia, se hace que las todas columnas tengan valores entre 0 y 1.\n",
        "\n",
        "\n",
        "\n",
        "3- ¿Por qué se separan los datos en set de entrenamiento y set de pruebas? ¿Qué proporción de los datos utilizaste para cada uno y por qué?\n",
        "\n",
        " Se separan porque la idea es que los programas puedan entrenarse con un set y después ponerlo a prueba con otro set que nunca ha visto, para que de verdad sea una predicción y no sólo repita información. En el clasificador al comienzo se usó un 70% de entrenamiento y 30% de testeo, sin embargo, al analizar los resultados se concluyó que se necesitaba de más datos de entrenamiento para poder mejorar el acurracy. Entonces se cambió a un 80% de entrenamiento y 20% de testeo, partición que es muy común. Para el regresor se hizo un 70% de entrenamiento y 30% de testeo, porque no había diferencia entre el MSE de la partición 70/30 y 80/20\n",
        "\n",
        "4- ¿Qué hiperparámetros modificaste para probar tu clasificador? ¿Y el regresor? ¿Cuáles combinaciones de parámetros te dieron mejores resultados y por qué crees que es así?\n",
        "\n",
        " Se modificó el número de vecinos a usar (n), el peso (weight) y el algoritmo. Se vio que n=50, weight='uniform' y algorithm='auto' maximizaba el acurracy, creemos que es porque al ser tantas publishers se necesitaba una muestra relativamente grande de puntos, y como los puntos eran tan dispersos no importaban tanto sus distancias con respecto al punto que se quiere clasificar. Para el regresor se cambió el fit_intercept, nomalize y n_jobs. Siempre se mantuvo el MSE. Esto tiene sentido, porque los datos ya venían normalizados.\n",
        "\n",
        "5- Para el clasificador, explica la diferencia entre las métricas del set de pruebas para cada clase, ¿Qué nos dice de la calidad del clasificador por cada clase?. Justifica.\n",
        "\n",
        " En el resultado del clasificador, se ve que muchos publishers nunca se predijeron. Analizando la predicción hecha con el set de prueba, se concluye que la calidad del clasificador es mala, tienendo un acurracy de solo 0.2. Esto pasa porque hay una gran cantidad de publishers, y la mayoría de estos tienen muy poca información disponible, lo que dificulta el poder tener buenos puntos de comparación.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6- Al usar t-SNE reducimos la dimensionalidad de los datos. Nombra dos razones para hacer eso.\n",
        "\n",
        " Una razón es que, al tener menos componentes por dato estos son mas fáciles de procesar y almacenar, ya que cualquier cálculo que se haga sobre los datos va a tener menos variables y se va a demorar menos. Otra razón es que al haber menos variables por dato, los algoritmos que se le apliquen, sobre todo los usados en ML, van a poder funcionar mejor, al operar sobre datos de menor complejidad.\n",
        "\n",
        "7- ¿Cuantos centroides elegiste para la división del set de datos? Justifica.\n",
        "\n",
        "  Se elijieron 578 centros, ya que lo que se buscaba es que el programa predijiera el Publisher de cada juego, por lo que al haber 578 Publisher el objetivo seria encontrar 578 Clusters, 1 por cada Publisher.\n",
        "\n",
        "8- ¿Que observaste al graficar los puntos luego de reducir su dimensionalidad? ¿Servirá hacer clustering para este set de datos?\n",
        "\n",
        "  Se observa que una gran parte de los datos estan dispersos de manera casi aleatortia, mientras que otros siguen una especie de linea continua. Esto nos mustra que el clustering no es ideal, por los datos dispersos. Ademas los datos mas continuos no pertenecen a un mismo Publisher, por lo que un si un cluster los clasifica juntos, por su cercania, se estaria equivocando."
      ]
    }
  ]
}